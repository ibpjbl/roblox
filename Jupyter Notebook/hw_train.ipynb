{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop вручную"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "\n",
    "* [Andrew Karpahy: yes, you should understand backprop.](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)\n",
    "* [Stanford CS231n](http://cs231n.stanford.edu/)\n",
    "* [Deep Learning](http://sereja.me/f/deep_learning_goodfellow.pdf) с 204 страницы и до прозрения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суть первой домашки — написать полноценную нейросеть чисто на numpy. \n",
    "\n",
    "Дизайн вдохновлен Torch-ем. Во всех современных eager-execution фреймворках примерно такой же интерфейс.\n",
    "\n",
    "Это нужно сделать ровно один раз в жизни — чтобы понять, как это работает изнутри. Через неделю уже будем использовать фреймворки, которые это всё считают за нас."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Проверяйте градиенты численно — сдвигайте параметры на какой-нибудь эпсилон и смотрите разницу\n",
    "* Пишите код без циклов — в питоне они очень долгие; все вычисления можно делать внутри numpy\n",
    "* Обсуждайте решение, но не шарьте друг другу код — так не интересно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откройте в Jupyter две тетрадки — эту и `hw_modules.ipynb`. В этой содержится train loop, а там непосредственно ваш «фреймворк», который вам ещё предстоит написать. Там есть комменты — они облегчат вам задачу.\n",
    "\n",
    "* Каждый модуль должен **возвращать** и **хранить** `output` и `gradInput`\n",
    "* Считайте, что `module.backward` всегда выполняется после `module.forward`, который сохранит в себе `output`\n",
    "\n",
    "Аналогично языковой модели с отбора, если придумаете какой-то более клёвый дизайн — вы вольны использовать его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run hw_modules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот вам немного уродливая имплементация SGD. Она нужна чисто для того, чтобы показать, как хранить состояние оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_momentum(x, dx, config, state):\n",
    "    \"\"\"\n",
    "        config — это\n",
    "            - momentum\n",
    "            - learning_rate\n",
    "        state:\n",
    "            - old_grad\n",
    "    \"\"\"\n",
    "    \n",
    "    state.setdefault('old_grad', {})\n",
    "    \n",
    "    i = 0 \n",
    "    for cur_layer_x, cur_layer_dx in zip(x, dx): \n",
    "        for cur_x, cur_dx in zip(cur_layer_x, cur_layer_dx):\n",
    "            cur_old_grad = state['old_grad'].setdefault(i, np.zeros_like(cur_dx))\n",
    "            np.add(config['momentum'] * cur_old_grad, config['learning_rate'] * cur_dx, out=cur_old_grad)\n",
    "            cur_x -= cur_old_grad\n",
    "            i += 1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Игрушечный пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с логистической регрессии на синтезированных данных.\n",
    "\n",
    "В этой секции ничего менять не надо — она нужна, чтобы отдебажить ваши слои в `hw_modules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 500\n",
    "\n",
    "X1 = np.random.randn(N, 2) + np.array([2, 2])\n",
    "X2 = np.random.randn(N, 2) + np.array([-2, -2])\n",
    "X = np.vstack([X1,X2])\n",
    "\n",
    "Y = np.concatenate([np.ones(N),np.zeros(N)])[:,None]\n",
    "Y = np.hstack([Y, 1-Y])\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1], c = Y[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия — это тоже как бы маленькая нейронка: линейный слой и софтмакс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "net.add(Linear(2, 2))\n",
    "net.add(SoftMax())\n",
    "\n",
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_config = {'learning_rate' : 1e-1, 'momentum': 0.9}\n",
    "optimizer_state = {}\n",
    "\n",
    "n_epoch = 20\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(dataset, batch_size):\n",
    "    'Эта функция позволит итерироваться по датасету.'\n",
    "    \n",
    "    X, Y = dataset\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # в начале каждой эпохи будем всё перемешивать\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_idx = indices[start:end]\n",
    "    \n",
    "        yield X[batch_idx], Y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простой цикл обучения. Очень важно его осознать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for x_batch, y_batch in get_batches((X, Y), batch_size):\n",
    "        net.zeroGradParameters()\n",
    "        \n",
    "        # forward — считаем все значения до функции потерь\n",
    "        predictions = net.forward(x_batch)\n",
    "        loss = criterion.forward(predictions, y_batch)\n",
    "    \n",
    "        # backward — считаем все градиенты в обратном порядке\n",
    "        dp = criterion.backward(predictions, y_batch)\n",
    "        net.backward(x_batch, dp)\n",
    "        \n",
    "        # обновляем веса\n",
    "        sgd_momentum(net.getParameters(), \n",
    "                     net.getGradParameters(), \n",
    "                     optimizer_config,\n",
    "                     optimizer_state)      \n",
    "        \n",
    "        loss_history.append(loss)\n",
    "\n",
    "    \n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_history, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь сами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MNIST](http://yann.lecun.com/exdb/mnist/) — это как дрозофилы в эволюционной биологии или коты в квантовой физике. \n",
    "\n",
    "Стандартный датасет для классификации, на котором все всё тестируют."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_mldata\n",
    "# эти библиотеки нужны только для того, чтобы его скачать\n",
    "\n",
    "if os.path.exists('mnist.npz'):\n",
    "    with np.load('mnist.npz', 'r') as data:\n",
    "        X = data['X']\n",
    "        y = data['y']\n",
    "else:\n",
    "    mnist = fetch_mldata(\"mnist-original\")\n",
    "    X = mnist.data / 255.0 # очень важно его отнормировать\n",
    "    y = mnist.target\n",
    "    np.savez('mnist.npz', X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведите лейблы в one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите датасет на train и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишите модель и train loop. Можете начать адаптации предыдущего примера.\n",
    "\n",
    "Дальше начинается творческая часть и настоящий Deep Learning:\n",
    "* Поиграйтесь с архитектурами\n",
    "* Поиграйтесь с learning rate и batch_size\n",
    "* Сделайте learning rate decay\n",
    "* Сделайте data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите точность. Она должна быть около 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
